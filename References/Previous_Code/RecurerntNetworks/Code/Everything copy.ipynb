{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'radio_data.npz'\n",
    "\n",
    "f = np.load(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train  = 40000\n",
    "\n",
    "x_train,x_test = f['traces'][:n_train],f['traces'][n_train:]\n",
    "signals = f['signals']\n",
    "labels = (signals.std(axis=-1) != 0).astype(float)  # define training label (1=cosmic event, 0=noise)\n",
    "y_train, y_test = labels[:n_train], labels[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = x_train.std()\n",
    "x_train /= sigma\n",
    "x_test /= sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 500)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (lstm1): LSTM(1, 32, batch_first=True, bidirectional=True)\n",
      "  (lstm2): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (lstm3): LSTM(128, 10, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=10000, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers, dropout_rate, output_size, sequence_length):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        # Hidden dimensions\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Number of hidden layers and bidirectionality\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], num_layers, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_sizes[0]*2, hidden_sizes[1], num_layers, batch_first=True, bidirectional=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_sizes[1]*2, hidden_sizes[2], num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Calculate feature size\n",
    "        self.feature_size = sequence_length * hidden_sizes[2] * 2\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.feature_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "sequence_length = 500  # Length of your sequences\n",
    "input_size = 1  # Number of features\n",
    "hidden_sizes = [32, 64, 10]  # Number of hidden units in the LSTM layers\n",
    "num_layers = 1  # Number of LSTM layers\n",
    "dropout_rate = 0.3  # Dropout rate after the final LSTM layer\n",
    "output_size = 1  # Number of output units\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BiLSTM(input_size, hidden_sizes, num_layers, dropout_rate, output_size, sequence_length).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model2, self).__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.LSTM_b_1 = nn.LSTM(1, 32, 1, batch_first=True, bidirectional=True)\n",
    "        self.LSTM_b_2 = nn.LSTM(64, 64, 1, batch_first=True, bidirectional=True)\n",
    "        self.LSTM_b_3 = nn.LSTM(128, 10, 1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.FC = nn.Linear(500*20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.LSTM_b_1(x)\n",
    "        out, _ = self.LSTM_b_2(out)\n",
    "        out, _ = self.LSTM_b_3(out)\n",
    "        out = out.contiguous().view(out.size(0), -1)  # Flatten\n",
    "        out = F.dropout(out, p=0.3)  # Dropout\n",
    "        out = self.FC(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.0019955048221680853 \tValidation Loss: 0.0019877799078822137\n",
      "Epoch: 2 \tTraining Loss: 0.0019695439661542575 \tValidation Loss: 0.0020250103026628495\n",
      "Epoch: 3 \tTraining Loss: 0.0019646609922250114 \tValidation Loss: 0.0020366621538996698\n",
      "Epoch: 4 \tTraining Loss: 0.001954938246972031 \tValidation Loss: 0.002070267729461193\n",
      "Epoch: 5 \tTraining Loss: 0.0019440842121839523 \tValidation Loss: 0.002072449065744877\n",
      "Epoch: 6 \tTraining Loss: 0.0019392280826965968 \tValidation Loss: 0.0020970431193709374\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch: 7 \tTraining Loss: 0.0019357089259558254 \tValidation Loss: 0.0021138175055384636\n",
      "Epoch: 8 \tTraining Loss: 0.001882757211724917 \tValidation Loss: 0.002131369970738888\n",
      "Epoch: 9 \tTraining Loss: 0.0018729489834772217 \tValidation Loss: 0.0021407899037003517\n",
      "Epoch: 10 \tTraining Loss: 0.0018762283482485348 \tValidation Loss: 0.0021508644968271256\n",
      "Epoch: 11 \tTraining Loss: 0.0018630236362417538 \tValidation Loss: 0.0021760375648736956\n",
      "Epoch: 12 \tTraining Loss: 0.0018555238420764606 \tValidation Loss: 0.0021785666942596435\n",
      "Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: 13 \tTraining Loss: 0.001853543423116207 \tValidation Loss: 0.0021833912283182144\n",
      "Epoch: 14 \tTraining Loss: 0.0018231986363728841 \tValidation Loss: 0.002177387237548828\n",
      "Epoch: 15 \tTraining Loss: 0.001816242559088601 \tValidation Loss: 0.0022244957238435744\n",
      "Epoch: 16 \tTraining Loss: 0.0018179936193757588 \tValidation Loss: 0.002220204405486584\n",
      "Epoch: 17 \tTraining Loss: 0.0018020454239514139 \tValidation Loss: 0.002198272630572319\n",
      "Epoch: 18 \tTraining Loss: 0.0018014955810374683 \tValidation Loss: 0.0022183579951524732\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch: 19 \tTraining Loss: 0.0017976214273108377 \tValidation Loss: 0.0022322027385234832\n",
      "Epoch: 20 \tTraining Loss: 0.0017827061456110742 \tValidation Loss: 0.0022619795501232147\n",
      "Epoch: 21 \tTraining Loss: 0.0017801248737507396 \tValidation Loss: 0.0022388047575950623\n",
      "Epoch: 22 \tTraining Loss: 0.0017878203433420922 \tValidation Loss: 0.0022480930387973786\n",
      "Epoch: 23 \tTraining Loss: 0.0017762334520618121 \tValidation Loss: 0.002223241314291954\n",
      "Epoch: 24 \tTraining Loss: 0.0017783768441942004 \tValidation Loss: 0.0022563865035772324\n",
      "Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch: 25 \tTraining Loss: 0.0017738559502694343 \tValidation Loss: 0.0022634913325309753\n",
      "Epoch: 26 \tTraining Loss: 0.0017554565775725576 \tValidation Loss: 0.0022763094902038573\n",
      "Epoch: 27 \tTraining Loss: 0.0017701531905266973 \tValidation Loss: 0.0022742269188165665\n",
      "Epoch: 28 \tTraining Loss: 0.0017642266990409957 \tValidation Loss: 0.0022659658938646316\n",
      "Epoch: 29 \tTraining Loss: 0.0017606862493687206 \tValidation Loss: 0.0022420141845941543\n",
      "Epoch: 30 \tTraining Loss: 0.001757587951918443 \tValidation Loss: 0.002260099470615387\n",
      "Epoch 00031: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch: 31 \tTraining Loss: 0.001750895280804899 \tValidation Loss: 0.0022530703246593474\n",
      "Epoch: 32 \tTraining Loss: 0.001762425207429462 \tValidation Loss: 0.0022582031041383744\n",
      "Epoch: 33 \tTraining Loss: 0.0017550018479426702 \tValidation Loss: 0.002242458090186119\n",
      "Epoch: 34 \tTraining Loss: 0.0017520105300678148 \tValidation Loss: 0.0022718082070350646\n",
      "Epoch: 35 \tTraining Loss: 0.0017498732407887777 \tValidation Loss: 0.0022744379341602325\n",
      "Epoch: 36 \tTraining Loss: 0.0017503903516464764 \tValidation Loss: 0.002269368439912796\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch: 37 \tTraining Loss: 0.0017463230192661286 \tValidation Loss: 0.0022770370990037917\n",
      "Epoch: 38 \tTraining Loss: 0.0017475155832038986 \tValidation Loss: 0.0022750819623470306\n",
      "Epoch: 39 \tTraining Loss: 0.001738522880607181 \tValidation Loss: 0.0022555752098560333\n",
      "Epoch: 40 \tTraining Loss: 0.0017464167277018228 \tValidation Loss: 0.0022589532434940337\n",
      "Epoch: 41 \tTraining Loss: 0.0017406331275900205 \tValidation Loss: 0.0022794580310583116\n",
      "Epoch: 42 \tTraining Loss: 0.0017531820237636567 \tValidation Loss: 0.002278274096548557\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 43 \tTraining Loss: 0.0017481702864170074 \tValidation Loss: 0.0022506165206432342\n",
      "Epoch: 44 \tTraining Loss: 0.0017513747687141101 \tValidation Loss: 0.002297134980559349\n",
      "Epoch: 45 \tTraining Loss: 0.0017437360998657015 \tValidation Loss: 0.00227120328694582\n",
      "Epoch: 46 \tTraining Loss: 0.001739200001789464 \tValidation Loss: 0.002265621319413185\n",
      "Epoch: 47 \tTraining Loss: 0.0017477268833253118 \tValidation Loss: 0.002246285915374756\n",
      "Epoch: 48 \tTraining Loss: 0.0017436696911851564 \tValidation Loss: 0.00226191296428442\n",
      "Epoch: 49 \tTraining Loss: 0.001746470298204157 \tValidation Loss: 0.00225629523396492\n",
      "Epoch: 50 \tTraining Loss: 0.0017523907174666722 \tValidation Loss: 0.0022864144295454024\n",
      "Epoch: 51 \tTraining Loss: 0.0017476187679502698 \tValidation Loss: 0.002273214489221573\n",
      "Epoch: 52 \tTraining Loss: 0.0017507043522265223 \tValidation Loss: 0.002260958880186081\n",
      "Epoch: 53 \tTraining Loss: 0.001745988859070672 \tValidation Loss: 0.002283234789967537\n",
      "Epoch: 54 \tTraining Loss: 0.0017440941234429677 \tValidation Loss: 0.002286191001534462\n",
      "Epoch: 55 \tTraining Loss: 0.001744211776389016 \tValidation Loss: 0.0022794953882694243\n",
      "Epoch: 56 \tTraining Loss: 0.0017371309449275335 \tValidation Loss: 0.0022816254794597625\n",
      "Epoch: 57 \tTraining Loss: 0.0017411881188551584 \tValidation Loss: 0.0023064348995685577\n",
      "Epoch: 58 \tTraining Loss: 0.001744113182855977 \tValidation Loss: 0.00229329489171505\n",
      "Epoch: 59 \tTraining Loss: 0.0017394627779722214 \tValidation Loss: 0.0022778397500514986\n",
      "Epoch: 60 \tTraining Loss: 0.0017425674655371242 \tValidation Loss: 0.0022658259868621826\n",
      "Epoch: 61 \tTraining Loss: 0.0017456028833985329 \tValidation Loss: 0.0022601911872625353\n",
      "Epoch: 62 \tTraining Loss: 0.0017430454856819577 \tValidation Loss: 0.0022835696488618853\n",
      "Epoch: 63 \tTraining Loss: 0.0017400796165068944 \tValidation Loss: 0.002277871564030647\n",
      "Epoch: 64 \tTraining Loss: 0.0017481079954240056 \tValidation Loss: 0.0022934765964746475\n",
      "Epoch: 65 \tTraining Loss: 0.0017384778923458524 \tValidation Loss: 0.002256866917014122\n",
      "Epoch: 66 \tTraining Loss: 0.001745992720954948 \tValidation Loss: 0.0022784442603588106\n",
      "Epoch: 67 \tTraining Loss: 0.0017459879236088858 \tValidation Loss: 0.002280232861638069\n",
      "Epoch: 68 \tTraining Loss: 0.001745394456717703 \tValidation Loss: 0.0022770567536354066\n",
      "Epoch: 69 \tTraining Loss: 0.0017359443513883484 \tValidation Loss: 0.0022871233969926834\n",
      "Epoch: 70 \tTraining Loss: 0.0017468970119953156 \tValidation Loss: 0.0022826684415340423\n",
      "Epoch: 71 \tTraining Loss: 0.0017330789301130507 \tValidation Loss: 0.0022623966783285143\n",
      "Epoch: 72 \tTraining Loss: 0.0017398916367027495 \tValidation Loss: 0.0022669891715049744\n",
      "Epoch: 73 \tTraining Loss: 0.00173841172705094 \tValidation Loss: 0.002280387997627258\n",
      "Epoch: 74 \tTraining Loss: 0.0017331511982613139 \tValidation Loss: 0.002280382938683033\n",
      "Epoch: 75 \tTraining Loss: 0.001741109001967642 \tValidation Loss: 0.00224894742667675\n",
      "Epoch: 76 \tTraining Loss: 0.001734364092350006 \tValidation Loss: 0.002283075898885727\n",
      "Epoch: 77 \tTraining Loss: 0.0017366296723484993 \tValidation Loss: 0.002280779629945755\n",
      "Epoch: 78 \tTraining Loss: 0.0017382014128896925 \tValidation Loss: 0.0023094325810670852\n",
      "Epoch: 79 \tTraining Loss: 0.0017406935708390342 \tValidation Loss: 0.0022877192199230196\n",
      "Epoch: 80 \tTraining Loss: 0.0017392348705066575 \tValidation Loss: 0.0022867453545331955\n",
      "Epoch: 81 \tTraining Loss: 0.0017360025197267533 \tValidation Loss: 0.002289902552962303\n",
      "Epoch: 82 \tTraining Loss: 0.0017426440748903487 \tValidation Loss: 0.0022705183774232866\n",
      "Epoch: 83 \tTraining Loss: 0.0017420083698299197 \tValidation Loss: 0.0022844847589731216\n",
      "Epoch: 84 \tTraining Loss: 0.0017394202343291706 \tValidation Loss: 0.0022816079035401344\n",
      "Epoch: 85 \tTraining Loss: 0.0017401637641919984 \tValidation Loss: 0.002280159018933773\n",
      "Epoch: 86 \tTraining Loss: 0.0017371100551552242 \tValidation Loss: 0.002269572883844376\n",
      "Epoch: 87 \tTraining Loss: 0.0017321826666593552 \tValidation Loss: 0.0022648561671376227\n",
      "Epoch: 88 \tTraining Loss: 0.001738181924654378 \tValidation Loss: 0.0022897825837135317\n",
      "Epoch: 89 \tTraining Loss: 0.0017335306290123197 \tValidation Loss: 0.0022557180523872376\n",
      "Epoch: 90 \tTraining Loss: 0.0017392018536726633 \tValidation Loss: 0.0022703017443418505\n",
      "Epoch: 91 \tTraining Loss: 0.0017346348787347476 \tValidation Loss: 0.002291907548904419\n",
      "Epoch: 92 \tTraining Loss: 0.001733201098938783 \tValidation Loss: 0.0023030468970537186\n",
      "Epoch: 93 \tTraining Loss: 0.0017424267638060781 \tValidation Loss: 0.002269150644540787\n",
      "Epoch: 94 \tTraining Loss: 0.0017326678757866223 \tValidation Loss: 0.0023028199076652526\n",
      "Epoch: 95 \tTraining Loss: 0.001738945403860675 \tValidation Loss: 0.002279118299484253\n",
      "Epoch: 96 \tTraining Loss: 0.0017395348184638553 \tValidation Loss: 0.002274492159485817\n",
      "Epoch: 97 \tTraining Loss: 0.001734108111096753 \tValidation Loss: 0.002280383788049221\n",
      "Epoch: 98 \tTraining Loss: 0.0017347239893343715 \tValidation Loss: 0.002281684398651123\n",
      "Epoch: 99 \tTraining Loss: 0.0017320492847098246 \tValidation Loss: 0.002274998739361763\n",
      "Epoch: 100 \tTraining Loss: 0.00173178997139136 \tValidation Loss: 0.002309106856584549\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Prepare data\n",
    "x_train_torch = torch.tensor(x_train[..., np.newaxis], dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Split train and validation data\n",
    "x_train_torch, x_val_torch, y_train_torch, y_val_torch = train_test_split(x_train_torch, y_train_torch, test_size=0.1)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(x_train_torch, y_train_torch), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(x_val_torch, y_val_torch), batch_size=256, shuffle=False)\n",
    "\n",
    "# Define model, optimizer and loss function\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define learning rate scheduler and early stopping parameters\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=5, verbose=True, min_lr=1e-5)\n",
    "n_epochs_stop = 15\n",
    "epochs_no_improve = 0\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    model.train()  # put model in training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Normalizing the loss by the total number of train batches\n",
    "    running_loss /= len(x_train_torch)\n",
    "    # print(f'Training Loss: {running_loss}')\n",
    "\n",
    "    # Calculate validation loss\n",
    "    val_loss = 0.0\n",
    "    model.eval()  # put model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels.unsqueeze(1)).item()\n",
    "\n",
    "    # Normalizing the loss by the total number of val batches\n",
    "    val_loss /= len(x_val_torch)\n",
    "    # print(f'Validation Loss: {val_loss}')\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_losses.append(running_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch+1} \\tTraining Loss: {running_loss} \\tValidation Loss: {val_loss}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
