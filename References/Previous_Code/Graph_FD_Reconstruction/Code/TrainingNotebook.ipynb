{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "os.system('clear')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# import torch.utils.data as data\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../Models/')\n",
    "sys.path.append('/remote/tychodata/ftairli/work/Projecs/Common/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset, assuming its already been calculated\n",
    "if 'ProcDS' not in globals():\n",
    "    ProcDS = torch.load('/remote/tychodata/ftairli/work/Projects/ProfileReconstruction/Data/XmaxE_Conv3d_Dataset.pt')\n",
    "    ProcDS.AssignIndices()\n",
    "\n",
    "ProcDS.BatchSize = 64\n",
    "ProcDS.RandomIter = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.zeros([10000,1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Some Paths\n",
    "SavePath     = '/remote/tychodata/ftairli/work/Projects/ProfileReconstruction/Models/'\n",
    "plotSavePath = '/remote/tychodata/ftairli/work/Projects/ProfileReconstruction/Results/TrainingPlots/'\n",
    "LogPath      = '/remote/tychodata/ftairli/work/Projects/TrainingLogs/'\n",
    "\n",
    "if plotSavePath != None:  # Purge the directory\n",
    "        os.system(f'rm -r {plotSavePath}')\n",
    "        os.system(f'mkdir {plotSavePath}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and axuillary functions\n",
    "ModelPath = '/remote/tychodata/ftairli/work/Projects/ProfileReconstruction/Models/'\n",
    "sys.path.append(ModelPath)\n",
    "\n",
    "import TrainingModule\n",
    "importlib.reload(TrainingModule)\n",
    "Tracker = TrainingModule.Tracker\n",
    "Train   = TrainingModule.Train\n",
    "\n",
    "\n",
    "\n",
    "import Model_XmaxE_Conv as ModelsFile\n",
    "importlib.reload(ModelsFile)\n",
    "\n",
    "Loss_function = ModelsFile.Loss\n",
    "validate      = ModelsFile.validate\n",
    "metric        = ModelsFile.metric\n",
    "\n",
    "Model         = ModelsFile.Model_XmaxE_Conv_3d_Distances_JustXmax\n",
    "\n",
    "\n",
    "# Setup the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "Model_Parameters = {\n",
    "    'in_main_channels': (3,),\n",
    "    'in_node_channels': 5   ,\n",
    "    'in_edge_channels': 2   ,\n",
    "    'in_aux_channels' : 1   ,\n",
    "    'N_kernels'       : 2   ,\n",
    "    'N_Graph_Heads'   : 2   ,\n",
    "    'N_dense_nodes'   : 16  ,\n",
    "    'N_LSTM_nodes'    : 64  ,\n",
    "    'N_LSTM_layers'   : 5   ,\n",
    "    'kernel_size'     : 10  ,\n",
    "    'conv2d_init_type': 'normal',\n",
    "    'model_Dropout'   : 0.2\n",
    "    }\n",
    "\n",
    "\n",
    "Training_Parameters = {\n",
    "        'LR': 0.0001,\n",
    "        'epochs': 5,\n",
    "        'BatchSize': ProcDS.BatchSize, # In notebook set during loading\n",
    "        'accumulation_steps': 1,\n",
    "        'epoch_done': 0,\n",
    "        'batchBreak': 1e99,\n",
    "        'ValLossIncreasePatience': 15,\n",
    "        'Optimizer': 'Adam'\n",
    "    }\n",
    "\n",
    "\n",
    "model = Model(**Model_Parameters).to(device)\n",
    "\n",
    "if Training_Parameters['Optimizer'] == 'Adam': optimizer = optim.Adam(model.parameters(), lr=Training_Parameters['LR'])\n",
    "if Training_Parameters['Optimizer'] == 'SGD' : optimizer = optim.SGD (model.parameters(), lr=Training_Parameters['LR'], momentum=0.9)\n",
    "# Define scheduler\n",
    "gamma = 0.001**(1/30) if Training_Parameters['epochs']>30 else 0.001**(1/Training_Parameters['epochs']) # Reduce the LR by factor of 1000 over 30 epochs or less\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR    (optimizer, gamma = gamma, last_epoch=-1, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Early Exit for Testingss: 1.1340\n",
      "\n",
      "Calculating Val Metrics\n",
      "torch.Size([512, 2])\n",
      "torch.Size([512, 2])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model,tracker \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mProcDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mLoss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mTracker\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mplotOnEpochCompletionPath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mTraining_Parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTraining_Parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mModel_Parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModel_Parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mLogPath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLogPath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/Projects/ProfileReconstruction/Code/TrainingModule.py:310\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(model, Dataset, optimiser, scheduler, Loss, Validation, Metric, Tracker, Training_Parameters, Model_Parameters, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Validation and Early stopping # metric is a str to be printed\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# val_losses  = Validation(model,Dataset,Loss,device)\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# val_loss    = val_losses['Total'] # Needed for scheduler step below\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculating Val Metrics\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 310\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mMetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# Scheduler Step is done here\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m Min_Val_Loss\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtolerance):\n",
      "File \u001b[0;32m~/work/Projects/ProfileReconstruction/Code/../Models/Model_XmaxE_Conv.py:102\u001b[0m, in \u001b[0;36mmetric\u001b[0;34m(model, Dataset, device, keys, BatchSize)\u001b[0m\n\u001b[1;32m    100\u001b[0m         metrics[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantile(torch\u001b[38;5;241m.\u001b[39mabs(AngDiv),\u001b[38;5;241m0.68\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m180\u001b[39m\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpi\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m         metrics[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantile(torch\u001b[38;5;241m.\u001b[39mabs(\u001b[43mPreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m-\u001b[39mTruths[:,i]),\u001b[38;5;241m0.68\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Return Batch Size to old value\u001b[39;00m\n\u001b[1;32m    104\u001b[0m Dataset\u001b[38;5;241m.\u001b[39mBatchSize \u001b[38;5;241m=\u001b[39m TrainingBatchSize\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model,tracker = Train(model,ProcDS,optimizer,scheduler,Loss_function,validate,metric ,Tracker,device = device,\\\n",
    "                      plotOnEpochCompletionPath=None,Training_Parameters=Training_Parameters,Model_Parameters=Model_Parameters,LogPath=LogPath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function Unnormalise_Xmax_Energy at 0x7f9b39275f70>\n"
     ]
    }
   ],
   "source": [
    "print(ProcDS.Unnormalise_Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "247\n",
      "120\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "def New_Lout(Lin,Kernel,Stride,Padding,dilation):\n",
    "    '''\n",
    "    Takes Lin,Kernel,Stride,Padding,dilation\n",
    "    Returns the output size of the Conv1d layer\n",
    "    '''\n",
    "    return int((Lin + 2*Padding - dilation*(Kernel-1) - 1)/Stride + 1)\n",
    "\n",
    "\n",
    "KernelSize = 10\n",
    "Stride     = 2 \n",
    "Padding    = 1\n",
    "dilation   = 1\n",
    "\n",
    "Lout = 500\n",
    "print(Lout)\n",
    "Lout = New_Lout(Lout,KernelSize,Stride,Padding,dilation)\n",
    "print(Lout)\n",
    "Lout = New_Lout(Lout,KernelSize,Stride,Padding,dilation)\n",
    "print(Lout)\n",
    "Lout = New_Lout(Lout,KernelSize,Stride,Padding,dilation)\n",
    "print(Lout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: long int != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m Timing   \u001b[38;5;241m=\u001b[39m AllNodes[:,\u001b[38;5;241m2\u001b[39m]                       \u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Graph Convolution\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m out\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAllNodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAllEdges\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAllEdgeV\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Do the Summary by masking edges with NBL >1\u001b[39;00m\n\u001b[1;32m     42\u001b[0m MaskedEdges \u001b[38;5;241m=\u001b[39m AllEdges\u001b[38;5;241m.\u001b[39mT[(AllEdgeV[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/conv/gat_conv.py:213\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, Tensor):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatic graphs not supported in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGATConv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 213\u001b[0m     x_src \u001b[38;5;241m=\u001b[39m x_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_src\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Tuple of source and target node features:\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     x_src, x_dst \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py:130\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: long int != float"
     ]
    }
   ],
   "source": [
    "ProcDS.BatchSize = 1\n",
    "from torch_geometric.nn import max_pool_x\n",
    "\n",
    "out = []\n",
    "\n",
    "for _,BatchGraph,BatchAux,BatchTruth,_ in ProcDS:\n",
    "    device = model.Chi01.weight.device\n",
    "        \n",
    "    # UnloadGraph\n",
    "    AllNodes = []\n",
    "    AllEdges = []\n",
    "    AllEdgeV = []\n",
    "    # EvSize   = [] # EvSize is no longer required\n",
    "    Batching = []\n",
    "    TotalNNodes = 0\n",
    "    \n",
    "\n",
    "    for BatchI,(Nodes,Edges,EdgeV) in enumerate(BatchGraph):\n",
    "        AllNodes.append(Nodes)\n",
    "        AllEdges.append(Edges+TotalNNodes)\n",
    "        AllEdgeV.append(EdgeV)\n",
    "        \n",
    "        # EvSize.append(len(torch.unique(Nodes[:,2]))) # number of unique time steps in the event\n",
    "        Batching.append(torch.ones(Nodes.shape[0])*BatchI)\n",
    "        TotalNNodes += Nodes.shape[0]\n",
    "\n",
    "\n",
    "    AllNodes = torch.cat(AllNodes,dim=0).to(device)\n",
    "    AllEdges = torch.cat(AllEdges,dim=0).to(device).T\n",
    "    AllEdgeV = torch.cat(AllEdgeV,dim=0).to(device)\n",
    "\n",
    "    # Node Info\n",
    "    # EvSize   = torch.tensor(EvSize)     .to(device).requires_grad_(False)\n",
    "    Batching = torch.cat(Batching,dim=0).to(device).requires_grad_(False).to(torch.long)\n",
    "    Timing   = AllNodes[:,2]                       .requires_grad_(False).to(torch.long)\n",
    "\n",
    "    \n",
    "    # Graph Convolution\n",
    "    out.append(model.Graph1(AllNodes,AllEdges,edge_attr=AllEdgeV))\n",
    "    \n",
    "    # Do the Summary by masking edges with NBL >1\n",
    "    MaskedEdges = AllEdges.T[(AllEdgeV[:,0] > 1)].T\n",
    "    MaskedEdgeV = AllEdgeV[(AllEdgeV[:,0] > 1)]\n",
    "    out.append(model.GraphS(out[-1],MaskedEdges,edge_attr=MaskedEdgeV))\n",
    "    Max_out,_ = max_pool_x(cluster = Timing, x = out[-1], batch = Batching, size = 500)\n",
    "    \n",
    "    # Reshape for Conv1d\n",
    "    out.append(Max_out.view(-1,out[-1].shape[1],500))\n",
    "    print(out[-1].shape)\n",
    "    \n",
    "    out.append(model.ConvActivation(model.Conv1(out[-1])))\n",
    "    print(out[-1].shape)\n",
    "    out.append(model.ConvActivation(model.Conv2(out[-1])))\n",
    "    print(out[-1].shape)\n",
    "    out.append(model.ConvActivation(model.Conv3(out[-1])))\n",
    "    print(out[-1].shape)\n",
    "    \n",
    "    Max_out = model.MaxPool(out[-1])\n",
    "    print(Max_out.shape)\n",
    "    # Dense Layers\n",
    "    out.append(Max_out.view(Max_out[-1].shape[0],-1))\n",
    "    print(out[-1].shape)\n",
    "    \n",
    "    # Output Layers\n",
    "    Chi0 = model.DenseActivation(model.Chi01(out[-1] ))\n",
    "    Chi0 = model.DenseActivation(model.Chi02(Chi0))\n",
    "    Chi0 = model.AngleActivation(model.Chi03(Chi0))\n",
    "    \n",
    "    Rp = model.DenseActivation(model.Rp1(out[-1]))\n",
    "    Rp = model.DenseActivation(model.Rp2(Rp ))\n",
    "    Rp =                      model.Rp3(Rp )\n",
    "\n",
    "    T0 = model.DenseActivation(model.T01(out[-1]))\n",
    "    T0 = model.DenseActivation(model.T02(T0 ))\n",
    "    T0 =                      model.T03(T0 )\n",
    "\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
