{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('/remote/tychodata/ftairli/work/Projects/ProfileReconstruction/Code/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.nn.pool import max_pool_x\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Dataset2 as Dataset\n",
    "\n",
    "importlib.reload(Dataset)\n",
    "DatasetContainer = Dataset.DatasetContainer\n",
    "ProcessingDatasetContainer = Dataset.ProcessingDatasetContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcDS = torch.load('/remote/tychodata/ftairli/work/Projects/ProfileReconstruction/Data/Axis_Conv3d_Dataset.pt')\n",
    "from DataGenFunctions import Unnormalise_Axis_Xmax_Energy\n",
    "\n",
    "\n",
    "ProcDS.Unnormalise_Truth = Unnormalise_Axis_Xmax_Energy\n",
    "\n",
    "torch.save(ProcDS, '/remote/tychodata/ftairli/work/Projects/ProfileReconstruction/Data/Axis_Conv3d_Dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ProcDS' not in globals():\n",
    "    ProcDS = torch.load('/remote/tychodata/ftairli/work/Projects/ProfileReconstruction/Data/Axis_Conv3d_Dataset.pt')\n",
    "    ProcDS.AssignIndices()\n",
    "\n",
    "ProcDS.BatchSize = 64\n",
    "ProcDS.RandomIter = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_Geom_Graph_C(\n",
       "  (Graph1): GATConv(5, 5, heads=16)\n",
       "  (Graph2): GATConv(80, 5, heads=16)\n",
       "  (Graph3): GATConv(80, 5, heads=16)\n",
       "  (GraphS): GATConv(80, 5, heads=16)\n",
       "  (Conv1): Conv1d(160, 16, kernel_size=(10,), stride=(2,), padding=(1,))\n",
       "  (Conv2): Conv1d(16, 16, kernel_size=(10,), stride=(2,), padding=(1,))\n",
       "  (Conv3): Conv1d(16, 16, kernel_size=(10,), stride=(2,), padding=(1,))\n",
       "  (ConvActivation): LeakyReLU(negative_slope=0.01)\n",
       "  (AvgPool): AvgPool1d(kernel_size=(10,), stride=(10,), padding=(0,))\n",
       "  (MaxPool): MaxPool1d(kernel_size=10, stride=10, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Dense1): Linear(in_features=352, out_features=128, bias=True)\n",
       "  (Dense2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (Dense3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (Chi01): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (Chi02): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (Chi03): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (Rp1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (Rp2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (Rp3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (T01): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (T02): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (T03): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (DenseActivation): LeakyReLU(negative_slope=0.01)\n",
       "  (AngleActivation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Model_Geom_Graph as Models\n",
    "importlib.reload(Models)\n",
    "Model_Geom_Graph_C = Models.Model_Geom_Graph_C\n",
    "model = Model_Geom_Graph_C()\n",
    "\n",
    "loss = Models.Loss\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "tensor(1.7970, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train for a bit\n",
    "Batches_to_compute = 1000\n",
    "ProcDS.BatchSize = 40\n",
    "\n",
    "for i,(_,BatchGraph, BatchAux, BatchTruth, _) in enumerate(ProcDS):\n",
    "    print(i, end='\\r')\n",
    "    if i == Batches_to_compute: break\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    preds = model(BatchGraph, BatchAux)\n",
    "    losses = loss(preds, BatchTruth)\n",
    "    totalLoss = losses['Total']\n",
    "    totalLoss.backward()\n",
    "    optimizer.step()\n",
    "print()\n",
    "print(totalLoss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077301248.0\n",
      "\n",
      "tensor([[  4.5166, -12.5192, -27.7353,  ...,  33.0060,  87.0572,  -5.1908],\n",
      "        [  4.7468, -12.5463, -27.6503,  ...,  33.0952,  86.7947,  -5.1267],\n",
      "        [  4.6157, -12.5388, -27.7070,  ...,  33.0326,  86.9485,  -5.1822],\n",
      "        ...,\n",
      "        [ 37.9396, -25.8989, -32.4871,  ...,  51.0374,  84.8433,  -9.9767],\n",
      "        [ 37.9389, -25.8821, -32.4613,  ...,  51.0441,  84.8156,  -9.9456],\n",
      "        [ 38.0024, -25.9121, -32.4803,  ...,  51.0623,  84.8045,  -9.9721]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -7.9087,   4.8784,\n",
      "         -3.1165,   3.8174,   3.8550,   5.6613,  -3.4987,  -5.6647,   9.4547,\n",
      "          0.2636,   1.1695,   1.8118,  -2.8269,  -4.9765,   4.3804,  -0.7623,\n",
      "          7.2788,  -4.8550,  -0.9650,   3.6473,  -5.6246,  -1.4748,  -4.3824,\n",
      "          7.7760,   3.7063, -11.3520,   3.1415,  -5.1861,  -2.8927,  -5.2346,\n",
      "          2.0763,   1.7117,   0.8752,   2.3977,  -5.3265,   6.5913, -10.2151,\n",
      "          5.4370,  -1.3379,  -8.8664,  -3.4760,  -2.8880,   0.2717,   6.6722,\n",
      "         -1.0603,   6.0819,   4.9343,   0.3991,   6.9716,  -1.5380,  -4.5814,\n",
      "          6.2180,   2.5328,   2.8504,  -3.1405,  -5.4609,   8.4960,  -6.3366,\n",
      "         -2.4994,   4.1587,  -6.1907,  -2.6397,   0.8925,  -2.4450,   3.3895,\n",
      "         -1.6213,  -1.1364,  -8.1458,   0.0151,  -0.7773,  -1.7907,   2.3069,\n",
      "         -4.0912,   2.4069,   4.3423,   0.1939,   5.0539,  -4.7702,  -0.0953,\n",
      "         -2.5527,  -7.9275,   4.8905,  -3.1133,   3.8089,   3.8629,   5.6180,\n",
      "         -3.4806,  -5.6593,   9.4277,   0.3022,   1.1758,   1.7923,  -2.8503,\n",
      "         -4.9633,   4.3914,  -0.7322,   7.2775,  -4.8393,  -0.9381,   3.6585,\n",
      "         -5.5847,  -1.4521,  -4.3949,   7.7717,   3.7157, -11.3623,   3.1538,\n",
      "         -5.1435,  -2.8871,  -5.2141,   2.0572,   1.6685,   0.8918,   2.3600,\n",
      "         -5.3255,   6.6105, -10.2276,   5.4198,  -1.3039,  -8.8259,  -3.4696,\n",
      "         -2.8906,   0.2391,   6.6830,  -1.0373,   6.0709,   4.9475,   0.3908,\n",
      "          6.9622,  -1.5536,  -4.5795,   6.1937,   2.5455,   2.8374,  -3.1447,\n",
      "         -5.4346,   8.4836,  -6.3167,  -2.5026,   4.1514,  -6.2046,  -2.6428,\n",
      "          0.9102,  -2.4539,   3.3701,  -1.6261,  -1.1050,  -8.1420,   0.0593,\n",
      "         -0.7602,  -1.7682,   2.2918,  -4.0952,   2.3821,   4.3233,   0.1972,\n",
      "          5.0372,  -4.7510,  -0.1432,  -2.5370,  -7.9032,   4.8805,  -3.1242,\n",
      "          3.8149,   3.8511,   5.6403,  -3.4822,  -5.6600,   9.4455,   0.2768,\n",
      "          1.1659,   1.8163,  -2.8085,  -4.9709,   4.3834,  -0.7631,   7.2563,\n",
      "         -4.8496,  -0.9572,   3.6400,  -5.6275,  -1.4668,  -4.3895,   7.7641,\n",
      "          3.7143, -11.3570,   3.1394,  -5.1808,  -2.8762,  -5.2399,   2.0717,\n",
      "          1.7011,   0.8694,   2.3943,  -5.3176,   6.5936, -10.2141,   5.4367,\n",
      "         -1.3374,  -8.8362,  -3.4703,  -2.8861,   0.2776,   6.6566,  -1.0701,\n",
      "          6.0870,   4.9449,   0.3832,   6.9698,  -1.5268,  -4.5742,   6.2150,\n",
      "          2.5250,   2.8296,  -3.1272,  -5.4336,   8.4815,  -6.3256,  -2.4968,\n",
      "          4.1543,  -6.1713,  -2.6444,   0.9045,  -2.4307,   3.3893,  -1.6302,\n",
      "         -1.1452,  -8.1303,   0.0363,  -0.7550,  -1.7719,   2.3006,  -4.0537,\n",
      "          2.3956,   4.3065,   0.1992,   5.0428,  -4.7616,  -0.0978,  -2.5478,\n",
      "         -7.9275,   4.8905,  -3.1133,   3.8089,   3.8629,   5.6180,  -3.4806,\n",
      "         -5.6593,   9.4277,   0.3022,   1.1758,   1.7923,  -2.8503,  -4.9633,\n",
      "          4.3914,  -0.7322,   7.2775,  -4.8393,  -0.9381,   3.6585,  -5.5847,\n",
      "         -1.4521,  -4.3949,   7.7717,   3.7157, -11.3623,   3.1538,  -5.1435,\n",
      "         -2.8871,  -5.2141,   2.0572,   1.6685,   0.8918,   2.3600,  -5.3255,\n",
      "          6.6105, -10.2276,   5.4198,  -1.3039,  -8.8259,  -3.4696,  -2.8906,\n",
      "          0.2391,   6.6830,  -1.0373,   6.0709,   4.9475,   0.3908,   6.9622,\n",
      "         -1.5536,  -4.5795,   6.1937,   2.5455,   2.8374,  -3.1447,  -5.4346,\n",
      "          8.4836,  -6.3167,  -2.5026,   4.1514,  -6.2046,  -2.6428,   0.9102,\n",
      "         -2.4539,   3.3701,  -1.6261,  -1.1050,  -8.1420,   0.0593,  -0.7602,\n",
      "         -1.7682,   2.2918,  -4.0952,   2.3821,   4.3233,   0.1972,   5.0372,\n",
      "         -4.7510,  -0.1432,  -2.5370,  -7.8982,   4.8823,  -3.1316,   3.8126,\n",
      "          3.8475,   5.6201,  -3.4661,  -5.6555,   9.4376,   0.2895,   1.1628,\n",
      "          1.8200,  -2.7919,  -4.9657,   4.3863,  -0.7641,   7.2320,  -4.8437,\n",
      "         -0.9489,   3.6321,  -5.6319,  -1.4556,  -4.3988,   7.7483,   3.7258,\n",
      "        -11.3624,   3.1371,  -5.1751,  -2.8585,  -5.2455,   2.0661,   1.6901,\n",
      "          0.8640,   2.3904,  -5.3088,   6.5965, -10.2131,   5.4362,  -1.3371,\n",
      "         -8.8035,  -3.4659,  -2.8846,   0.2828,   6.6424,  -1.0784,   6.0919,\n",
      "          4.9563,   0.3680,   6.9684,  -1.5154,  -4.5674,   6.2118,   2.5187,\n",
      "          2.8105,  -3.1152,  -5.4051,   8.4661,  -6.3133,  -2.4935,   4.1493,\n",
      "         -6.1499,  -2.6498,   0.9179,  -2.4149,   3.3893,  -1.6397,  -1.1546,\n",
      "         -8.1141,   0.0578,  -0.7312,  -1.7569,   2.2958,  -4.0241,   2.3865,\n",
      "          4.2780,   0.2045,   5.0322,  -4.7532,  -0.1002,  -2.5427,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote/tychodata/ftairli/.local/lib/python3.9/site-packages/torch_geometric/warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Step by step training\n",
    "Batches_to_compute = 1\n",
    "ProcDS.BatchSize = 1\n",
    "for i,(EvId,BatchGraph, BatchAux, BatchTruth, _) in enumerate(ProcDS):\n",
    "    print(EvId.item())\n",
    "    print()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(BatchGraph, BatchAux,True)\n",
    "    break\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
