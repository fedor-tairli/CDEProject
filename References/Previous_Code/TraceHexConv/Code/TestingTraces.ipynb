{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/remote/tychodata/ftairli/work/Projects/TraceHexConv/Models/')\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import gc\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "L = 120\n",
    "C = 3\n",
    "H = 11\n",
    "W = 11\n",
    "d_main = torch.rand((N,L,C,H,W),dtype=torch.float32,device='cuda')\n",
    "# d_main = d_main.permute(0, 3, 4, 1, 2).reshape(N, H * W, L, C).reshape(N * H * W, L, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trace_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Trace_Block, self).__init__()\n",
    "\n",
    "        self.bi_lstm = nn.LSTM(input_size=3, \n",
    "                                hidden_size=10, \n",
    "                                num_layers=2, \n",
    "                                batch_first=True, \n",
    "                                bidirectional=True)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=20, # 2 for bidirection\n",
    "                            hidden_size=10,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Read parameters of input\n",
    "        # N, L, C, H, W = x.shape\n",
    "        # N = 10\n",
    "        # L = 120\n",
    "        # C = 3\n",
    "        # H = 11\n",
    "        # W = 11\n",
    "\n",
    "        # # Flatten spatial dimensions and transpose to (N, H*W, L, C) and Reshape again to (N*H*W, L, C)\n",
    "        x = x.permute(0, 3, 4, 1, 2).reshape(N, H * W, L, C).reshape(N * H * W, L, C)\n",
    "\n",
    "        \n",
    "\n",
    "        # Now you can process all traces in parallel\n",
    "        out, _ = self.bi_lstm(x)  # Shape = N*H*W, L, 2*hidden_size\n",
    "        out, _ = self.lstm(out)   # Shape = N*H*W, L, hidden_size\n",
    "        out = out[:, -1, :]       # Shape = N*H*W, hidden_size\n",
    "\n",
    "        # Finally, reshape and permute the output to the desired shape\n",
    "        out = out.reshape(N, H, W, -1).permute(0, 3, 1, 2)  # Shape = N, hidden_size, H, W\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recurrent_Block(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=10, num_layers=1, dropout_rate=0.5, num_features=10):\n",
    "        super(Recurrent_Block, self).__init__()\n",
    "\n",
    "        # Bidirectional LSTM layer\n",
    "        self.bidirectional_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(hidden_dim*2, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "\n",
    "        # Linear layer to transform output to desired number of features\n",
    "        self.fc = nn.Linear(hidden_dim, num_features)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.bidirectional_lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: (batch_size, sequence_length, num_channels, width, height)\n",
    "\n",
    "        batch_size, sequence_length, num_channels, width, height = x.shape\n",
    "\n",
    "        # rearrange input to shape: (batch_size*width*height, sequence_length, num_channels)\n",
    "        x = x.permute(0, 3, 4, 1, 2).contiguous().view(-1, sequence_length, num_channels)\n",
    "\n",
    "        # pass data through Bidirectional LSTM layer\n",
    "        bidir_lstm_out, _ = self.bidirectional_lstm(x)  # output shape: (batch_size*width*height, sequence_length, hidden_dim*2)\n",
    "\n",
    "        # pass data through LSTM layers\n",
    "        lstm_out, _ = self.lstm(bidir_lstm_out)  # output shape: (batch_size*width*height, sequence_length, hidden_dim)\n",
    "\n",
    "        # apply linear layer to every time step\n",
    "        features = self.fc(lstm_out[:, -1, :])  # output shape: (batch_size*width*height, num_features)\n",
    "\n",
    "        # reshape features to original width and height, shape: (batch_size, height, width, num_features)\n",
    "        features = features.view(batch_size, -1, width, height)\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Block(nn.Module):\n",
    "    def __init__(self, d_model=360, nhead=4, num_layers=1, dim_feedforward=512, dropout=0.1,num_features=10):\n",
    "        super(Transformer_Block, self).__init__()\n",
    "\n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # Linear layer to transform output to desired number of features\n",
    "        self.fc = nn.Linear(d_model,num_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input shape: (batch_size, sequence_length, num_channels, width, height)\n",
    "\n",
    "        batch_size, sequence_length, num_channels, width, height = x.shape\n",
    "\n",
    "        # rearrange input to shape: (batch_size*width*height, sequence_length*num_channels)\n",
    "        x = x.permute(0, 3, 4, 1, 2).contiguous().view(-1, sequence_length * num_channels)\n",
    "\n",
    "        # pass data through Transformer layers\n",
    "        transformer_out = self.transformer_encoder(x)  # output shape: (batch_size*width*height, sequence_length, d_model)\n",
    "\n",
    "        # apply linear layer to every time step\n",
    "        features = self.fc(transformer_out)  # output shape: (batch_size*width*height, sequence_length, d_model)\n",
    "\n",
    "        # reshape features to original width and height, and permute dimensions\n",
    "        # final shape: (batch_size, sequence_length*d_model, height, width)\n",
    "        features = features.view(batch_size, width, height, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer_Block().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-07-05 22:13:09 22753:22753 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------    ------------  ------------  ------------  \n",
      "                                                   Name  m      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------    ------------  ------------  ------------  \n",
      "                                               [memory]         5.12 Gb       5.12 Gb            59  \n",
      "                                       cudaLaunchKernel             0 b           0 b            23  \n",
      "void at::native::elementwise_kernel<128, 2, void at:...             0 b           0 b             2  \n",
      "                                  cudaStreamIsCapturing             0 b           0 b            14  \n",
      "                                             cudaMalloc             0 b           0 b            13  \n",
      "                                               cudaFree             0 b           0 b             2  \n",
      "                                 cudaDeviceGetAttribute             0 b           0 b            14  \n",
      "                                   cudaGetSymbolAddress             0 b           0 b             1  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...             0 b           0 b            24  \n",
      "                                 ampere_sgemm_128x64_tn             0 b           0 b             6  \n",
      "-------------------------------------------------------    ------------  ------------  ------------  \n",
      "Self CPU time total: 506.685ms\n",
      "Self CUDA time total: 45.628ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-07-05 22:13:09 22753:22753 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-07-05 22:13:09 22753:22753 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "[W collection.cpp:700] Warning: Failed to recover relationship between all profiler and kineto events: 59 vs. 0  reassociated. (function reassociate)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "with profile(activities=[ ProfilerActivity.CUDA],profile_memory=True, record_shapes=True) as prof:\n",
    "    \n",
    "    predictions = model(d_main)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "strings = ((prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))).split('\\n')\n",
    "len1 = len('-------------------------------------------------------  ')\n",
    "len2 = len('-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------')\n",
    "len3 = len('')\n",
    "for s in strings:\n",
    "    print(s[:len1],end='')\n",
    "    print(s[len2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10, 11, 11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
